{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllEditor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        return [line.strip() for line in lines]\n",
    "\n",
    "    def view_annotations(self):\n",
    "        annotations = [line for line in self.data if line and not line.startswith(\"-DOCSTART-\")]\n",
    "        annotation_count = len(annotations)  # Count the total number of annotations\n",
    "        for annotation in annotations:\n",
    "            print(annotation)\n",
    "        print(f\"\\nTotal number of tokens: {annotation_count}\")\n",
    "    \n",
    "    def label_stats(self):\n",
    "        label_counter = Counter()\n",
    "        total_labels = 0  # Variable to keep track of the total number of labels\n",
    "        \n",
    "        for line in self.data:\n",
    "            if line and not line.startswith(\"-DOCSTART-\"):\n",
    "                label = line.split()[-1]\n",
    "                label_counter[label] += 1\n",
    "                total_labels += 1  # Increment the total label count for each valid line\n",
    "\n",
    "        unique_labels = len(label_counter)  # Total number of unique tags\n",
    "        \n",
    "        for label, count in label_counter.items():\n",
    "            print(f\"Label: {label}, Count: {count}\")\n",
    "        \n",
    "        print(f\"\\nTotal number of labels found: {total_labels}\")\n",
    "        print(f\"Total number of unique tags: {unique_labels}\")\n",
    "\n",
    "    \n",
    "    def search_by_label(self, label):\n",
    "        matches = [line for line in self.data if line and not line.startswith(\"-DOCSTART-\") and line.endswith(label)]\n",
    "        token_count = len(matches)\n",
    "        \n",
    "        # Count sentences containing the label\n",
    "        sentence_count = 0\n",
    "        current_sentence_has_label = False\n",
    "        \n",
    "        for line in self.data:\n",
    "            if not line:  # Empty line indicates end of sentence\n",
    "                if current_sentence_has_label:\n",
    "                    sentence_count += 1\n",
    "                current_sentence_has_label = False\n",
    "            elif line.endswith(label):\n",
    "                current_sentence_has_label = True\n",
    "        \n",
    "        # Check the last sentence if it doesn't end with an empty line\n",
    "        if current_sentence_has_label:\n",
    "            sentence_count += 1\n",
    "        \n",
    "        # Print matches\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "        \n",
    "        print(f\"\\nNumber of tokens found with label '{label}': {token_count}\")\n",
    "        print(f\"Number of sentences containing label '{label}': {sentence_count}\")\n",
    "\n",
    "    def search_by_token(self, token):\n",
    "        matches = [line for line in self.data if line and not line.startswith(\"-DOCSTART-\") and token in line]\n",
    "        token_count = len(matches)\n",
    "\n",
    "        # Count sentences containing the token\n",
    "        sentence_count = 0\n",
    "        current_sentence_has_token = False\n",
    "\n",
    "        for line in self.data:\n",
    "            if not line:  # Empty line indicates end of sentence\n",
    "                if current_sentence_has_token:\n",
    "                    sentence_count += 1\n",
    "                current_sentence_has_token = False\n",
    "            elif token in line:\n",
    "                current_sentence_has_token = True\n",
    "\n",
    "        # Check the last sentence if it doesn't end with an empty line\n",
    "        if current_sentence_has_token:\n",
    "            sentence_count += 1\n",
    "\n",
    "        # Print matches\n",
    "        for match in matches:\n",
    "            print(match)\n",
    "\n",
    "        print(f\"\\nNumber of tokens found with '{token}': {token_count}\")\n",
    "        print(f\"Number of sentences containing '{token}': {sentence_count}\")\n",
    "\n",
    "    def remove_label(self, label_to_remove):\n",
    "        new_data = []\n",
    "        for line in self.data:\n",
    "            if line.endswith(label_to_remove):\n",
    "                new_data.append(re.sub(rf\"\\s{label_to_remove}$\", \" O\", line))\n",
    "            else:\n",
    "                new_data.append(line)\n",
    "        self.data = new_data\n",
    "        print(f\"Label '{label_to_remove}' removed.\")\n",
    "    \n",
    "    def merge_labels(self, labels_to_merge, new_label):\n",
    "        new_data = []\n",
    "        for line in self.data:\n",
    "            if any(line.endswith(label) for label in labels_to_merge):\n",
    "                new_data.append(re.sub(rf\"\\s({'|'.join(labels_to_merge)})$\", f\" {new_label}\", line))\n",
    "            else:\n",
    "                new_data.append(line)\n",
    "        self.data = new_data\n",
    "        print(f\"Labels {labels_to_merge} merged into '{new_label}'.\")\n",
    "    \n",
    "    def rename_labels(self, label_mapping):\n",
    "        new_data = []\n",
    "        for line in self.data:\n",
    "            if line and not line.startswith(\"-DOCSTART-\"):\n",
    "                parts = line.split()\n",
    "                if parts:\n",
    "                    label = parts[-1]\n",
    "                    if label in label_mapping:\n",
    "                        parts[-1] = label_mapping[label]\n",
    "                    new_data.append(\" \".join(parts))\n",
    "            else:\n",
    "                new_data.append(line)\n",
    "        self.data = new_data\n",
    "        print(f\"Labels renamed according to {label_mapping}.\")\n",
    "\n",
    "    def delete_sentences_with_label(self, label_to_delete):\n",
    "        new_data = []\n",
    "        current_sentence = []\n",
    "        sentence_to_delete = False\n",
    "        sentences_deleted = 0\n",
    "        tokens_deleted = 0\n",
    "\n",
    "        for line in self.data:\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                new_data.append(line)\n",
    "                continue\n",
    "\n",
    "            if not line:\n",
    "                if current_sentence and not sentence_to_delete:\n",
    "                    new_data.extend(current_sentence)\n",
    "                    new_data.append(line)\n",
    "                else:\n",
    "                    if sentence_to_delete:\n",
    "                        sentences_deleted += 1\n",
    "                        tokens_deleted += len(current_sentence)\n",
    "                current_sentence = []\n",
    "                sentence_to_delete = False\n",
    "            else:\n",
    "                current_sentence.append(line)\n",
    "                if line.split()[-1] == label_to_delete:\n",
    "                    sentence_to_delete = True\n",
    "\n",
    "        # Handle the last sentence if it exists\n",
    "        if current_sentence:\n",
    "            if sentence_to_delete:\n",
    "                sentences_deleted += 1\n",
    "                tokens_deleted += len(current_sentence)\n",
    "            else:\n",
    "                new_data.extend(current_sentence)\n",
    "\n",
    "        self.data = new_data\n",
    "        print(f\"Sentences containing the label '{label_to_delete}' have been deleted.\")\n",
    "        print(f\"Number of sentences deleted: {sentences_deleted}\")\n",
    "        print(f\"Number of tokens deleted: {tokens_deleted}\")\n",
    "\n",
    "    def delete_sentences_without_annotations(self):\n",
    "        new_data = []\n",
    "        current_sentence = []\n",
    "        sentence_has_annotation = False\n",
    "        sentences_deleted = 0\n",
    "        tokens_deleted = 0\n",
    "\n",
    "        for line in self.data:\n",
    "            if line.startswith(\"-DOCSTART-\"):\n",
    "                new_data.append(line)\n",
    "                continue\n",
    "\n",
    "            if not line:\n",
    "                if current_sentence and sentence_has_annotation:\n",
    "                    new_data.extend(current_sentence)\n",
    "                    new_data.append(line)\n",
    "                else:\n",
    "                    if current_sentence:\n",
    "                        sentences_deleted += 1\n",
    "                        tokens_deleted += len(current_sentence)\n",
    "                current_sentence = []\n",
    "                sentence_has_annotation = False\n",
    "            else:\n",
    "                current_sentence.append(line)\n",
    "                if line.split()[-1] != \"O\":\n",
    "                    sentence_has_annotation = True\n",
    "\n",
    "        # Handle the last sentence if it exists\n",
    "        if current_sentence:\n",
    "            if sentence_has_annotation:\n",
    "                new_data.extend(current_sentence)\n",
    "            else:\n",
    "                sentences_deleted += 1\n",
    "                tokens_deleted += len(current_sentence)\n",
    "\n",
    "        self.data = new_data\n",
    "        print(f\"Sentences without annotations have been deleted successfully.\")\n",
    "        print(f\"Number of sentences deleted: {sentences_deleted}\")\n",
    "        print(f\"Number of tokens deleted: {tokens_deleted}\")\n",
    "\n",
    "    def save(self, output_path):\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(self.data) + \"\\n\")\n",
    "        print(f\"Updated file saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the editor with the CoNLL file path\n",
    "editor = ConllEditor(r'c:\\Users\\Sakib Ahmed\\Desktop\\samples\\merged Final.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hulls -X- _ B-PER\n",
      "señaló -X- _ O\n",
      "que -X- _ O\n",
      "en -X- _ O\n",
      "el -X- _ O\n",
      "sistema -X- _ O\n",
      "...\n",
      "07,20 -X- _ O\n",
      ". -X- _ O\n",
      "\n",
      "Total number of tokens: 264715\n"
     ]
    }
   ],
   "source": [
    "# 1. View Annotations\n",
    "editor.view_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: B-Country, Count: 390\n",
      "Label: B-State, Count: 528\n",
      "Label: O, Count: 28221\n",
      "Label: B-Class, Count: 383\n",
      "Label: B-License_Number, Count: 592\n",
      "Label: I-License_Number, Count: 608\n",
      "Label: B-Last_Name, Count: 607\n",
      "Label: B-First_Name, Count: 613\n",
      "Label: I-First_Name, Count: 353\n",
      "Label: B-Address, Count: 694\n",
      "Label: I-Address, Count: 3546\n",
      "Label: B-Sex, Count: 423\n",
      "Label: B-Height, Count: 433\n",
      "Label: I-Height, Count: 949\n",
      "Label: B-Restrictions, Count: 347\n",
      "Label: B-Endorsement, Count: 348\n",
      "Label: B-DD, Count: 316\n",
      "Label: B-Document_Number, Count: 184\n",
      "Label: I-Last_Name, Count: 88\n",
      "Label: B-Eyes, Count: 410\n",
      "Label: B-Weight, Count: 245\n",
      "Label: I-Weight, Count: 137\n",
      "Label: B-Other, Count: 234\n",
      "Label: I-State, Count: 96\n",
      "Label: I-Restrictions, Count: 25\n",
      "Label: I-Eyes, Count: 28\n",
      "Label: I-DD, Count: 217\n",
      "Label: B-Hair, Count: 127\n",
      "Label: B-License_Type, Count: 30\n",
      "Label: I-Other, Count: 59\n",
      "Label: I-Endorsement, Count: 29\n",
      "Label: B-Race, Count: 1\n",
      "Label: I-License_Type, Count: 6\n",
      "Label: I-Document_Number, Count: 73\n",
      "Label: I-Hair, Count: 6\n",
      "Label: I-Sex, Count: 3\n",
      "Label: B-Birth_Place, Count: 111\n",
      "Label: I-Birth_Place, Count: 49\n",
      "Label: B-Authority, Count: 118\n",
      "Label: B-Issuance_Number, Count: 35\n",
      "Label: B-DL_Class, Count: 113\n",
      "Label: I-DL_Class, Count: 608\n",
      "Label: I-Authority, Count: 107\n",
      "Label: I-Country, Count: 13\n",
      "Label: I-Class, Count: 19\n",
      "Label: I-Issuance_Number, Count: 1\n",
      "\n",
      "Total number of labels found: 42523\n",
      "Total number of unique tags: 46\n"
     ]
    }
   ],
   "source": [
    "# 2. Label Statistics\n",
    "editor.label_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hulls -X- _ B-PER\n",
      "Areces -X- _ B-PER\n",
      "Juan -X- _ B-PER\n",
      "Jaime -X- _ B-PER\n",
      "Chevenement -X- _ B-PER\n",
      "Luis -X- _ B-PER\n",
      "Conchita -X- _ B-PER\n",
      "...\n",
      "F. -X- _ B-PER\n",
      "\n",
      "Number of tokens found with label 'B-PER': 4321\n",
      "Number of sentences containing label 'B-PER': 2666\n"
     ]
    }
   ],
   "source": [
    "# 3. Search Annotations with a specific label\n",
    "editor.search_by_label('B-PER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-Last_Name\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "Florida -X- _ B-State\n",
      "\n",
      "Number of tokens found with 'Florida': 13\n",
      "Number of sentences containing 'Florida': 13\n"
     ]
    }
   ],
   "source": [
    "# 4. Search Annotations with a specific label\n",
    "editor.search_by_token(\"Florida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'B-PER' removed.\n"
     ]
    }
   ],
   "source": [
    "# 5. Remove specific label\n",
    "editor.remove_label('B-PER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels ['B-MISC', 'I-MISC', 'B-ORG'] merged into 'C-MISC'.\n"
     ]
    }
   ],
   "source": [
    "# 6. Merge multiple labels into one\n",
    "editor.merge_labels(['B-MISC', 'I-MISC', 'B-ORG'], 'C-MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: O, Count: 236241\n",
      "Label: C-MISC, Count: 12775\n",
      "Label: B-LOC, Count: 4913\n",
      "Label: I-ORG, Count: 4992\n",
      "Label: I-LOC, Count: 1891\n",
      "Label: I-PER, Count: 3903\n",
      "\n",
      "Total number of labels found: 264715\n"
     ]
    }
   ],
   "source": [
    "# Rechecking Label Statistics\n",
    "editor.label_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels renamed according to {'I-PER': 'A-MISC', 'B-LOC': 'A-LOC'}.\n"
     ]
    }
   ],
   "source": [
    "# 7. Rename labels based on JSON mapping\n",
    "editor.rename_labels({\n",
    "    'I-PER':'A-MISC',\n",
    "    'B-LOC':'A-LOC'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: O, Count: 236241\n",
      "Label: C-MISC, Count: 12775\n",
      "Label: A-LOC, Count: 4913\n",
      "Label: I-ORG, Count: 4992\n",
      "Label: I-LOC, Count: 1891\n",
      "Label: A-MISC, Count: 3903\n",
      "\n",
      "Total number of labels found: 264715\n"
     ]
    }
   ],
   "source": [
    "# Rechecking Label Statistics\n",
    "editor.label_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences containing the label 'I-LOC' have been deleted.\n",
      "Number of sentences deleted: 413\n",
      "Number of tokens deleted: 14029\n"
     ]
    }
   ],
   "source": [
    "# 8. Delete an entire sentence containing an specific label\n",
    "editor.delete_sentences_with_label(\"I-LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences without annotations have been deleted successfully.\n",
      "Number of sentences deleted: 2123\n",
      "Number of tokens deleted: 31337\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# 9. Delete an entire sentence containing no label\n",
    "editor.delete_sentences_without_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated file saved to updated_conll_file.conll\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Save the updated CoNLL file\n",
    "editor.save('updated_conll_file.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-ASCII lines remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lines_with_gaps(file_path):\n",
    "    cleaned_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if line is either empty or contains only ASCII characters\n",
    "        if line.strip() == '' or re.match(r'^[\\x00-\\x7F]+$', line.strip()):\n",
    "            cleaned_lines.append(line)  # Keep the line\n",
    "\n",
    "    # Save cleaned lines to a new file\n",
    "    with open('output.conll', 'w', encoding='utf-8') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "    \n",
    "    print(\"Non-ASCII tokens removed successfully\")\n",
    "\n",
    "# Use the path to your .conll file here\n",
    "clean_lines_with_gaps(r'updated_conll_file.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates Remvover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "from io import StringIO\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = './data'\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, 'removed_duplicates')\n",
    "ALLOWED_EXTENSIONS = {'conll'}\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def get_next_file_number():\n",
    "    existing_files = glob.glob(os.path.join(OUTPUT_DIR, 'cleaned_*.conll'))\n",
    "    if not existing_files:\n",
    "        return 1\n",
    "    numbers = [int(f.split('_')[-1].split('.')[0]) for f in existing_files]\n",
    "    return max(numbers) + 1\n",
    "\n",
    "def parse_conll_content(content):\n",
    "    \"\"\"Parse CoNLL content into sentences\"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for line in content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip DOCSTART\n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            continue\n",
    "            \n",
    "        if line:\n",
    "            current_sentence.append(line)\n",
    "        elif current_sentence:  # Empty line and we have a sentence\n",
    "            sentences.append('\\n'.join(current_sentence))\n",
    "            current_sentence = []\n",
    "            \n",
    "    # Add last sentence if exists\n",
    "    if current_sentence:\n",
    "        sentences.append('\\n'.join(current_sentence))\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "def remove_duplicates(content):\n",
    "    \"\"\"Remove duplicate sentences and track removed ones\"\"\"\n",
    "    # Initialize output buffers\n",
    "    cleaned_output = StringIO()\n",
    "    removed_output = StringIO()\n",
    "    \n",
    "    # Add headers\n",
    "    cleaned_output.write('-DOCSTART- -X- O O\\n\\n')\n",
    "    removed_output.write('-DOCSTART- -X- O O\\n\\n')\n",
    "    \n",
    "    # Parse into sentences\n",
    "    sentences = parse_conll_content(content)\n",
    "    \n",
    "    # Track unique and duplicate sentences\n",
    "    seen_sentences = {}  # Hash -> First occurrence index\n",
    "    unique_sentences = []\n",
    "    removed_sentences = []\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Create hash of the sentence\n",
    "        sentence_hash = hashlib.md5(sentence.encode()).hexdigest()\n",
    "        \n",
    "        if sentence_hash not in seen_sentences:\n",
    "            seen_sentences[sentence_hash] = idx\n",
    "            unique_sentences.append(sentence)\n",
    "        else:\n",
    "            # Store duplicate with its position information\n",
    "            original_pos = seen_sentences[sentence_hash] + 1  # 1-based indexing\n",
    "            current_pos = idx + 1\n",
    "            removed_sentences.append((sentence, original_pos, current_pos))\n",
    "    \n",
    "    # Write unique sentences to cleaned output\n",
    "    for sentence in unique_sentences:\n",
    "        cleaned_output.write(sentence + '\\n\\n')\n",
    "    \n",
    "    # Write removed sentences to removed output with position information\n",
    "    for sentence, original_pos, current_pos in removed_sentences:\n",
    "        removed_output.write(f\"# Duplicate of sentence #{original_pos}, found at position #{current_pos}\\n\")\n",
    "        removed_output.write(sentence + '\\n\\n')\n",
    "    \n",
    "    return (\n",
    "        cleaned_output.getvalue(),\n",
    "        removed_output.getvalue(),\n",
    "        len(sentences),\n",
    "        len(unique_sentences),\n",
    "        len(removed_sentences)\n",
    "    )\n",
    "\n",
    "def main(file_path):\n",
    "    if not allowed_file(file_path):\n",
    "        print('Invalid file type. Only .conll files are allowed')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Read the content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Process the content\n",
    "        cleaned_content, removed_content, original_count, unique_count, removed_count = remove_duplicates(content)\n",
    "        \n",
    "        # Get next file number and create formatted number string\n",
    "        file_number = get_next_file_number()\n",
    "        formatted_number = f\"{file_number:04d}\"\n",
    "        \n",
    "        # Create output filenames\n",
    "        cleaned_filename = f\"cleaned_{formatted_number}.conll\"\n",
    "        removed_filename = f\"removed_sentences_{formatted_number}.conll\"\n",
    "        \n",
    "        # Create full paths\n",
    "        cleaned_path = os.path.join(OUTPUT_DIR, cleaned_filename)\n",
    "        removed_path = os.path.join(OUTPUT_DIR, removed_filename)\n",
    "        \n",
    "        # Save the files\n",
    "        with open(cleaned_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_content)\n",
    "            \n",
    "        with open(removed_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(removed_content)\n",
    "        \n",
    "        # Print success message with file information\n",
    "        print(f'File deduplicated successfully:')\n",
    "        print(f'Cleaned file saved to: {cleaned_path}')\n",
    "        print(f'Removed sentences file saved to: {removed_path}')\n",
    "        print(f'Statistics:')\n",
    "        print(f'Original sentences: {original_count}')\n",
    "        print(f'Unique sentences: {unique_count}')\n",
    "        print(f'Duplicates removed: {removed_count}')\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print('Invalid file encoding. File must be UTF-8 encoded')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {str(e)}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace 'path/to/your_file.conll' with the actual file path\n",
    "    main(r'c:\\Users\\Sakib Ahmed\\Downloads\\project-20-at-2024-10-31-22-29-f2d4705b.conll')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
